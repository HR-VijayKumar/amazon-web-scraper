{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c0912-1d7e-48b1-8b20-60f2498b4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests                    # For sending HTTP requests to web pages\n",
    "from bs4 import BeautifulSoup      # For parsing HTML content\n",
    "import pandas as pd                # For storing data in tabular format (DataFrame)\n",
    "import random                      # For selecting random user-agents to avoid being blocked\n",
    "import time                        # (Optional) Can be used to slow down requests to avoid detection\n",
    "\n",
    "# List of different user-agents (helps in mimicking browser behavior and reduces the chance of getting blocked)\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Randomly select a user-agent for the request\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "    \"Accept-Language\": \"en-Us,en;q=0.9\"\n",
    "}\n",
    "\n",
    "# Lists to store scraped data\n",
    "product_name = []\n",
    "product_price = []\n",
    "product_rating = []\n",
    "\n",
    "# URL of the Amazon product listing page (here: kitchen category, page 1)\n",
    "url = f\"https://www.amazon.in/s?i=kitchen&bbn=81107433031&rh=n%3A81107433031%2Cp_85%3A10440599031&page=1&_encoding=UTF8&content-id=amzn1.sym.58c90a12-100b-4a2f-8e15-7c06f1abe2be&pd_rd_r=eb705f4e-d34b-456d-a496-b52f6602d46b&pd_rd_w=hwFSy&pd_rd_wg=MVPlH&qid=1745483311&xpid=thnXwLcmHk4-q&ref=sr_pg_2\"\n",
    "\n",
    "# Send GET request to fetch HTML content\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all product links using their CSS class\n",
    "links = soup.find_all(\"a\", attrs={\"class\": \"a-link-normal s-line-clamp-4 s-link-style a-text-normal\"})\n",
    "\n",
    "# Loop through each product link\n",
    "for i, link in enumerate(links):\n",
    "    href = link.get('href')  # Extract relative product URL\n",
    "    product_link = \"https://www.amazon.in\" + href  # Form complete product URL\n",
    "\n",
    "    # Send GET request to product detail page\n",
    "    new_url = requests.get(product_link, headers=headers)\n",
    "    new_soup = BeautifulSoup(new_url.content, \"html.parser\")\n",
    "\n",
    "    # Extract product name, price, and rating using their unique tags/IDs/classes\n",
    "    name = new_soup.find(\"span\", attrs={\"id\": \"productTitle\"})\n",
    "    price = new_soup.find(\"span\", attrs={\"class\": \"a-price-whole\"})\n",
    "    rating = new_soup.find(\"span\", attrs={\"class\": \"a-icon-alt\"})\n",
    "\n",
    "    # Append data to lists (with fallback \"NA\" if data is missing)\n",
    "    product_name.append(name.text.strip() if name else \"NA\")\n",
    "    product_price.append(price.text.strip().rstrip('.') if price else \"NA\")\n",
    "    product_rating.append(rating.text.strip() if rating else \"NA\")\n",
    "\n",
    "# Print summary of total records scraped\n",
    "print(f\"Total number of records:\\n Product Names: {len(product_name)}\\n Prices: {len(product_price)}\\n Ratings: {len(product_rating)}\")\n",
    "\n",
    "# Create a DataFrame using the scraped data\n",
    "data = {\n",
    "    \"product_name\": product_name,\n",
    "    \"price\": product_price,\n",
    "    \"ratings\": product_rating\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"amazon_products_single_page.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed. Data saved to 'amazon_products_single_page.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1f2f0-54aa-49cb-9b6e-1f35b438237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows (optional for debugging)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
